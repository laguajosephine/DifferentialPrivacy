{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install opacus","metadata":{"execution":{"iopub.status.busy":"2025-03-10T15:41:56.702278Z","iopub.execute_input":"2025-03-10T15:41:56.702563Z","iopub.status.idle":"2025-03-10T15:42:02.152874Z","shell.execute_reply.started":"2025-03-10T15:41:56.702534Z","shell.execute_reply":"2025-03-10T15:42:02.151789Z"},"trusted":true,"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting opacus\n  Downloading opacus-1.5.3-py3-none-any.whl.metadata (8.4 kB)\nRequirement already satisfied: numpy<2.0,>=1.15 in /usr/local/lib/python3.10/dist-packages (from opacus) (1.26.4)\nRequirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from opacus) (2.5.1+cu121)\nRequirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.10/dist-packages (from opacus) (1.13.1)\nRequirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from opacus) (3.4.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.0,>=1.15->opacus) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.0,>=1.15->opacus) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.0,>=1.15->opacus) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.0,>=1.15->opacus) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.0,>=1.15->opacus) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.0,>=1.15->opacus) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0->opacus) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->opacus) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0,>=1.15->opacus) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.0,>=1.15->opacus) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.0,>=1.15->opacus) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.0,>=1.15->opacus) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.0,>=1.15->opacus) (2024.2.0)\nDownloading opacus-1.5.3-py3-none-any.whl (251 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.7/251.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: opacus\nSuccessfully installed opacus-1.5.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader, Subset\nfrom opacus import PrivacyEngine\nfrom torchvision.models import inception_v3, Inception_V3_Weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:42:11.683730Z","iopub.execute_input":"2025-03-10T15:42:11.684035Z","iopub.status.idle":"2025-03-10T15:42:18.207887Z","shell.execute_reply.started":"2025-03-10T15:42:11.684009Z","shell.execute_reply":"2025-03-10T15:42:18.207206Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Définir le device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Préparation des données CIFAR-10 avec redimensionnement adapté pour InceptionV3 (299x299)\ntransform = transforms.Compose([\n    transforms.Resize((299, 299)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\ntrainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntestset  = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\n# Création d'un déséquilibre de classes\nclass_sizes = {0: 5000, 1: 4500, 2: 4000, 3: 3000, 4: 2000, 5: 1000, 6: 800, 7: 700, 8: 600, 9: 500}\nclass_indices = {i: np.where(np.array(trainset.targets) == i)[0] for i in range(10)}\nimbalanced_indices = np.concatenate([np.random.choice(class_indices[i], size, replace=False)\n                                     for i, size in class_sizes.items()])\ntrain_subset = Subset(trainset, imbalanced_indices)\n\n# Vous pouvez augmenter la taille du batch pour améliorer la stabilité (surtout pour DP)\nbatch_size = 32\ntrain_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\ntest_loader  = DataLoader(testset, batch_size=batch_size, shuffle=False)\n\n# --- Création du modèle non-DP ---\n# Important : désactiver l'auxiliary classifier pour InceptionV3\n# Charger le modèle avec les poids pré-entraînés et désactiver aux_logits\nmodel = inception_v3(weights=None, aux_logits=False)\n\n# Charger manuellement les poids pré-entraînés\nweights = Inception_V3_Weights.DEFAULT\nstate_dict = weights.get_state_dict(progress=True)\nmodel.load_state_dict(state_dict, strict=False)  # Ne pas forcer les clés strictement\n\n# Modifier la dernière couche pour adapter CIFAR-10 (10 classes)\nmodel.fc = nn.Linear(2048, 10)\n\n# Envoyer le modèle sur le bon device (GPU/CPU)\nmodel = model.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Fonction d'entraînement générique\ndef train_model(model, loader, criterion, optimizer):\n    model.train()\n    running_loss = 0.0\n    for images, labels in loader:\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)  # Retourne directement un tenseur car aux_logits=False\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n    return running_loss / len(loader)\n\n# Fonction d'évaluation\ndef evaluate(model, loader):\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for images, labels in loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return correct / total\n\n# Évaluation par classe\ndef evaluate_per_class(model, loader):\n    model.eval()\n    correct = torch.zeros(10)\n    total = torch.zeros(10)\n    with torch.no_grad():\n        for images, labels in loader:\n            images = images.to(device)\n            labels = labels.to(device)\n            outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            for i in range(len(labels)):\n                total[labels[i]] += 1\n                if predicted[i] == labels[i]:\n                    correct[labels[i]] += 1\n    return (correct / total * 100).cpu().numpy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:42:21.114945Z","iopub.execute_input":"2025-03-10T15:42:21.115374Z","iopub.status.idle":"2025-03-10T15:42:28.532203Z","shell.execute_reply.started":"2025-03-10T15:42:21.115345Z","shell.execute_reply":"2025-03-10T15:42:28.531552Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170M/170M [00:02<00:00, 65.7MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/inception.py:43: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n  warnings.warn(\nDownloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n100%|██████████| 104M/104M [00:00<00:00, 228MB/s]  \n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Non-DP model","metadata":{}},{"cell_type":"code","source":"# --- Entraînement du modèle non-DP ---\nnum_epochs = 15\nprint(\"Entraînement du modèle non-DP\")\nfor epoch in range(num_epochs):\n    loss = train_model(model, train_loader, criterion, optimizer)\n    acc = evaluate(model, test_loader)\n    acc_per_class = evaluate_per_class(model, test_loader)\n    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {loss:.4f} - Global Accuracy: {acc*100:.2f}%\")\n    for i, class_acc in enumerate(acc_per_class):\n        print(f\"  Classe {i}: {class_acc:.2f}%\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DP- model","metadata":{}},{"cell_type":"code","source":"# --- Passage en mode DP ---\nnum_epochs = 15\n\n# Libérer la mémoire GPU si nécessaire\ntorch.cuda.empty_cache()\n\n# Fonction pour \"geler\" les BatchNorm (utile pour l'entraînement DP)\ndef freeze_batchnorm(model):\n    for module in model.modules():\n        if isinstance(module, nn.BatchNorm2d):\n            module.track_running_stats = False\n            module.weight.requires_grad = False\n            module.bias.requires_grad = False\n            # On peut aussi choisir de remplacer BatchNorm par GroupNorm dans une version plus aboutie\n\n# Création du modèle DP (idem, désactivation de aux_logits)\nmodel_dp = inception_v3(weights=None, aux_logits=False)\nweights = Inception_V3_Weights.DEFAULT\nstate_dict = weights.get_state_dict(progress=True)\nmodel_dp.load_state_dict(state_dict, strict=False)  # Ne pas forcer les clés strictement\nmodel_dp.fc = nn.Linear(2048, 10)\nfreeze_batchnorm(model_dp)  # Appliquer avant de passer sur le device\nmodel_dp = model_dp.to(device)\n\noptimizer_dp = optim.Adam(model_dp.parameters(), lr=0.001)\ncriterion_dp = nn.CrossEntropyLoss()\n\n# Création du PrivacyEngine avec des paramètres adaptés\n# Pour se rapprocher des réglages du papier, on peut essayer noise_multiplier=0.6 et max_grad_norm=1.0\nprivacy_engine = PrivacyEngine()\n\nmodel_dp, optimizer_dp, train_loader_dp = privacy_engine.make_private(\n    module=model_dp,\n    optimizer=optimizer_dp,\n    data_loader=train_loader,  # Vous pouvez utiliser le même loader ou en recréer un si nécessaire\n    noise_multiplier=0.4,       # Ajusté pour avoir un ε raisonnable (< 10)\n    max_grad_norm=1.2,\n)\n\n# Boucle d'entraînement DP\nprint(\"\\nEntraînement du modèle DP\")\nfor epoch in range(num_epochs):\n    model_dp.train()\n    running_loss = 0.0\n    for images, labels in train_loader_dp:\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        optimizer_dp.zero_grad()\n        outputs = model_dp(images)\n        loss = criterion_dp(outputs, labels)\n        loss.backward()\n        optimizer_dp.step()\n        \n        running_loss += loss.item()\n    \n    avg_loss = running_loss / len(train_loader_dp)\n    dp_acc = evaluate(model_dp, test_loader)\n    dp_acc_per_class = evaluate_per_class(model_dp, test_loader)\n    \n    # Affichage de la perte, de l'exactitude et du coût de confidentialité\n    epsilon = privacy_engine.get_epsilon(delta=1e-6)\n    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f} - DP Accuracy: {dp_acc*100:.2f}%\")\n    print(f\"(ε = {epsilon:.2f}, δ = 1e-6)\")\n    for i, class_acc in enumerate(dp_acc_per_class):\n        print(f\"  Classe {i}: {class_acc:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T15:42:36.140003Z","iopub.execute_input":"2025-03-10T15:42:36.140289Z","iopub.status.idle":"2025-03-10T15:57:08.498172Z","shell.execute_reply.started":"2025-03-10T15:42:36.140267Z","shell.execute_reply":"2025-03-10T15:57:08.497023Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/opacus/privacy_engine.py:96: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\nEntraînement du modèle DP\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1827: FutureWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n  self._maybe_warn_non_full_backward_hook(args, result, grad_fn)\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10] - Loss: 2.1039 - DP Accuracy: 9.98%\n(ε = 2.46, δ = 1e-6)\n  Classe 0: 99.70%\n  Classe 1: 0.10%\n  Classe 2: 0.00%\n  Classe 3: 0.00%\n  Classe 4: 0.00%\n  Classe 5: 0.00%\n  Classe 6: 0.00%\n  Classe 7: 0.00%\n  Classe 8: 0.00%\n  Classe 9: 0.00%\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-002159073193>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion_dp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0moptimizer_dp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/opacus/optimizers/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m                 \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moriginal_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/opacus/optimizers/optimizer.py\u001b[0m in \u001b[0;36mpre_step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    541\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_last_step_skipped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_hook\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/opacus/optimizers/optimizer.py\u001b[0m in \u001b[0;36madd_noise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0m_check_processed_flag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummed_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m             noise = _generate_noise(\n\u001b[0m\u001b[1;32m    471\u001b[0m                 \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise_multiplier\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m                 \u001b[0mreference\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummed_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/opacus/optimizers/optimizer.py\u001b[0m in \u001b[0;36m_generate_noise\u001b[0;34m(std, reference, generator, secure_mode)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0menough\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0man\u001b[0m \u001b[0mattacker\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m     \"\"\"\n\u001b[0;32m--> 137\u001b[0;31m     \u001b[0mzeros\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":4}]}